{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Cage6666/Big-Data-Application/blob/master/6-Neural%20Network/Neural%20Network.ipynb)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# 神经网络-多层感知机"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom collections import Counter\nimport math\nfrom math import log\n\nimport pprint\n\nfrom imblearn.combine import SMOTETomek\n\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\nfrom imblearn.combine import SMOTETomek\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":"train = pd.read_csv('https://raw.githubusercontent.com/Cage6666/Big-Data-Application/master/2-Classification/PPDtrain.csv',engine = 'python')\ntest = pd.read_csv('https://raw.githubusercontent.com/Cage6666/Big-Data-Application/master/2-Classification/PPDtest.csv',engine = 'python')"},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n","of pandas will change to not sort by default.\n","\n","To accept the future behavior, pass 'sort=False'.\n","\n","To retain the current behavior and silence the warning, pass 'sort=True'.\n","\n","  sort=sort)\n"]}],"source":"data = train.append(test)"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Education_Info1</th>\n","      <th>Education_Info5</th>\n","      <th>Idx</th>\n","      <th>SocialNetwork_1</th>\n","      <th>SocialNetwork_10</th>\n","      <th>SocialNetwork_11</th>\n","      <th>SocialNetwork_12</th>\n","      <th>SocialNetwork_13</th>\n","      <th>SocialNetwork_14</th>\n","      <th>SocialNetwork_15</th>\n","      <th>...</th>\n","      <th>WeblogInfo_54</th>\n","      <th>WeblogInfo_55</th>\n","      <th>WeblogInfo_56</th>\n","      <th>WeblogInfo_57</th>\n","      <th>WeblogInfo_58</th>\n","      <th>WeblogInfo_6</th>\n","      <th>WeblogInfo_7</th>\n","      <th>WeblogInfo_8</th>\n","      <th>WeblogInfo_9</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>...</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>28349.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.063000</td>\n","      <td>0.032467</td>\n","      <td>46318.673267</td>\n","      <td>0.001233</td>\n","      <td>75.211233</td>\n","      <td>-0.999267</td>\n","      <td>-0.745033</td>\n","      <td>0.221167</td>\n","      <td>0.062033</td>\n","      <td>0.027967</td>\n","      <td>...</td>\n","      <td>0.006167</td>\n","      <td>0.000800</td>\n","      <td>0.019067</td>\n","      <td>0.016133</td>\n","      <td>0.002633</td>\n","      <td>2.948711</td>\n","      <td>10.632800</td>\n","      <td>0.657633</td>\n","      <td>0.120567</td>\n","      <td>0.073267</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.242967</td>\n","      <td>0.177239</td>\n","      <td>26640.397805</td>\n","      <td>0.036948</td>\n","      <td>742.978305</td>\n","      <td>0.052911</td>\n","      <td>0.441473</td>\n","      <td>0.420545</td>\n","      <td>0.242598</td>\n","      <td>0.164880</td>\n","      <td>...</td>\n","      <td>0.115740</td>\n","      <td>0.036507</td>\n","      <td>0.183587</td>\n","      <td>0.169727</td>\n","      <td>0.063455</td>\n","      <td>3.770300</td>\n","      <td>16.097588</td>\n","      <td>2.622133</td>\n","      <td>1.337519</td>\n","      <td>0.260578</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>3.000000</td>\n","      <td>0.000000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>22924.250000</td>\n","      <td>0.000000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>46849.500000</td>\n","      <td>0.000000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","      <td>6.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>69447.250000</td>\n","      <td>0.000000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>3.000000</td>\n","      <td>13.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>91703.000000</td>\n","      <td>2.000000</td>\n","      <td>71253.000000</td>\n","      <td>6.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>3.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>11.000000</td>\n","      <td>3.000000</td>\n","      <td>11.000000</td>\n","      <td>11.000000</td>\n","      <td>3.000000</td>\n","      <td>165.000000</td>\n","      <td>722.000000</td>\n","      <td>81.000000</td>\n","      <td>46.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 211 columns</p>\n","</div>"],"text/plain":["       Education_Info1  Education_Info5           Idx  SocialNetwork_1  \\\n","count     30000.000000     30000.000000  30000.000000     30000.000000   \n","mean          0.063000         0.032467  46318.673267         0.001233   \n","std           0.242967         0.177239  26640.397805         0.036948   \n","min           0.000000         0.000000      3.000000         0.000000   \n","25%           0.000000         0.000000  22924.250000         0.000000   \n","50%           0.000000         0.000000  46849.500000         0.000000   \n","75%           0.000000         0.000000  69447.250000         0.000000   \n","max           1.000000         1.000000  91703.000000         2.000000   \n","\n","       SocialNetwork_10  SocialNetwork_11  SocialNetwork_12  SocialNetwork_13  \\\n","count      30000.000000      30000.000000      30000.000000      30000.000000   \n","mean          75.211233         -0.999267         -0.745033          0.221167   \n","std          742.978305          0.052911          0.441473          0.420545   \n","min           -1.000000         -1.000000         -1.000000          0.000000   \n","25%           -1.000000         -1.000000         -1.000000          0.000000   \n","50%           -1.000000         -1.000000         -1.000000          0.000000   \n","75%           -1.000000         -1.000000          0.000000          0.000000   \n","max        71253.000000          6.000000          1.000000          2.000000   \n","\n","       SocialNetwork_14  SocialNetwork_15  ...  WeblogInfo_54  WeblogInfo_55  \\\n","count      30000.000000      30000.000000  ...   30000.000000   30000.000000   \n","mean           0.062033          0.027967  ...       0.006167       0.000800   \n","std            0.242598          0.164880  ...       0.115740       0.036507   \n","min            0.000000          0.000000  ...       0.000000       0.000000   \n","25%            0.000000          0.000000  ...       0.000000       0.000000   \n","50%            0.000000          0.000000  ...       0.000000       0.000000   \n","75%            0.000000          0.000000  ...       0.000000       0.000000   \n","max            3.000000          1.000000  ...      11.000000       3.000000   \n","\n","       WeblogInfo_56  WeblogInfo_57  WeblogInfo_58  WeblogInfo_6  \\\n","count   30000.000000   30000.000000   30000.000000  28349.000000   \n","mean        0.019067       0.016133       0.002633      2.948711   \n","std         0.183587       0.169727       0.063455      3.770300   \n","min         0.000000       0.000000       0.000000      1.000000   \n","25%         0.000000       0.000000       0.000000      1.000000   \n","50%         0.000000       0.000000       0.000000      2.000000   \n","75%         0.000000       0.000000       0.000000      3.000000   \n","max        11.000000      11.000000       3.000000    165.000000   \n","\n","       WeblogInfo_7  WeblogInfo_8  WeblogInfo_9        target  \n","count  30000.000000  30000.000000  30000.000000  30000.000000  \n","mean      10.632800      0.657633      0.120567      0.073267  \n","std       16.097588      2.622133      1.337519      0.260578  \n","min        0.000000      0.000000      0.000000      0.000000  \n","25%        2.000000      0.000000      0.000000      0.000000  \n","50%        6.000000      0.000000      0.000000      0.000000  \n","75%       13.000000      0.000000      0.000000      0.000000  \n","max      722.000000     81.000000     46.000000      1.000000  \n","\n","[8 rows x 211 columns]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":"data.describe()"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":"#去掉超过95%列值为NaN或0的变量\nfinal_data = data.loc[:, (data==np.nan).mean() < .95]\nfinal_data = final_data.loc[:, (data==0).mean() < .95]\n#把余下少量的缺失值填为-99(一般会填为0或-1，但在许多列中存在0和-1，故这里选用-99)。\nfinal_data = final_data.fillna(-99)"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Education_Info1</th>\n","      <th>Idx</th>\n","      <th>SocialNetwork_10</th>\n","      <th>SocialNetwork_11</th>\n","      <th>SocialNetwork_12</th>\n","      <th>SocialNetwork_13</th>\n","      <th>SocialNetwork_14</th>\n","      <th>SocialNetwork_17</th>\n","      <th>SocialNetwork_3</th>\n","      <th>SocialNetwork_4</th>\n","      <th>...</th>\n","      <th>WeblogInfo_30</th>\n","      <th>WeblogInfo_33</th>\n","      <th>WeblogInfo_36</th>\n","      <th>WeblogInfo_39</th>\n","      <th>WeblogInfo_4</th>\n","      <th>WeblogInfo_5</th>\n","      <th>WeblogInfo_6</th>\n","      <th>WeblogInfo_7</th>\n","      <th>WeblogInfo_8</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>3.000000e+04</td>\n","      <td>30000.000000</td>\n","      <td>...</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","      <td>30000.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.063000</td>\n","      <td>46318.673267</td>\n","      <td>75.211233</td>\n","      <td>-0.999267</td>\n","      <td>-0.745033</td>\n","      <td>0.221167</td>\n","      <td>0.062033</td>\n","      <td>0.253467</td>\n","      <td>8.552363e+01</td>\n","      <td>5.014967</td>\n","      <td>...</td>\n","      <td>-0.696733</td>\n","      <td>-0.534800</td>\n","      <td>-0.643667</td>\n","      <td>-0.730933</td>\n","      <td>-2.588867</td>\n","      <td>-3.731333</td>\n","      <td>-2.661867</td>\n","      <td>10.632800</td>\n","      <td>0.657633</td>\n","      <td>0.073267</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.242967</td>\n","      <td>26640.397805</td>\n","      <td>742.978305</td>\n","      <td>0.052911</td>\n","      <td>0.441473</td>\n","      <td>0.420545</td>\n","      <td>0.242598</td>\n","      <td>0.437296</td>\n","      <td>6.362796e+03</td>\n","      <td>73.277866</td>\n","      <td>...</td>\n","      <td>9.077462</td>\n","      <td>9.155202</td>\n","      <td>9.102907</td>\n","      <td>9.071642</td>\n","      <td>23.554150</td>\n","      <td>23.050615</td>\n","      <td>23.536427</td>\n","      <td>16.097588</td>\n","      <td>2.622133</td>\n","      <td>0.260578</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>3.000000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-1.000000e+00</td>\n","      <td>-1.000000</td>\n","      <td>...</td>\n","      <td>-99.000000</td>\n","      <td>-99.000000</td>\n","      <td>-99.000000</td>\n","      <td>-99.000000</td>\n","      <td>-99.000000</td>\n","      <td>-99.000000</td>\n","      <td>-99.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.000000</td>\n","      <td>22924.250000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-1.000000e+00</td>\n","      <td>-1.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.000000</td>\n","      <td>46849.500000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-1.000000e+00</td>\n","      <td>-1.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>6.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.000000</td>\n","      <td>69447.250000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>-1.000000e+00</td>\n","      <td>-1.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>3.000000</td>\n","      <td>2.000000</td>\n","      <td>3.000000</td>\n","      <td>13.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>91703.000000</td>\n","      <td>71253.000000</td>\n","      <td>6.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>1.032721e+06</td>\n","      <td>2996.000000</td>\n","      <td>...</td>\n","      <td>12.000000</td>\n","      <td>21.000000</td>\n","      <td>23.000000</td>\n","      <td>12.000000</td>\n","      <td>165.000000</td>\n","      <td>73.000000</td>\n","      <td>165.000000</td>\n","      <td>722.000000</td>\n","      <td>81.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 171 columns</p>\n","</div>"],"text/plain":["       Education_Info1           Idx  SocialNetwork_10  SocialNetwork_11  \\\n","count     30000.000000  30000.000000      30000.000000      30000.000000   \n","mean          0.063000  46318.673267         75.211233         -0.999267   \n","std           0.242967  26640.397805        742.978305          0.052911   \n","min           0.000000      3.000000         -1.000000         -1.000000   \n","25%           0.000000  22924.250000         -1.000000         -1.000000   \n","50%           0.000000  46849.500000         -1.000000         -1.000000   \n","75%           0.000000  69447.250000         -1.000000         -1.000000   \n","max           1.000000  91703.000000      71253.000000          6.000000   \n","\n","       SocialNetwork_12  SocialNetwork_13  SocialNetwork_14  SocialNetwork_17  \\\n","count      30000.000000      30000.000000      30000.000000      30000.000000   \n","mean          -0.745033          0.221167          0.062033          0.253467   \n","std            0.441473          0.420545          0.242598          0.437296   \n","min           -1.000000          0.000000          0.000000          0.000000   \n","25%           -1.000000          0.000000          0.000000          0.000000   \n","50%           -1.000000          0.000000          0.000000          0.000000   \n","75%            0.000000          0.000000          0.000000          1.000000   \n","max            1.000000          2.000000          3.000000          3.000000   \n","\n","       SocialNetwork_3  SocialNetwork_4  ...  WeblogInfo_30  WeblogInfo_33  \\\n","count     3.000000e+04     30000.000000  ...   30000.000000   30000.000000   \n","mean      8.552363e+01         5.014967  ...      -0.696733      -0.534800   \n","std       6.362796e+03        73.277866  ...       9.077462       9.155202   \n","min      -1.000000e+00        -1.000000  ...     -99.000000     -99.000000   \n","25%      -1.000000e+00        -1.000000  ...       0.000000       0.000000   \n","50%      -1.000000e+00        -1.000000  ...       0.000000       0.000000   \n","75%      -1.000000e+00        -1.000000  ...       0.000000       0.000000   \n","max       1.032721e+06      2996.000000  ...      12.000000      21.000000   \n","\n","       WeblogInfo_36  WeblogInfo_39  WeblogInfo_4  WeblogInfo_5  WeblogInfo_6  \\\n","count   30000.000000   30000.000000  30000.000000  30000.000000  30000.000000   \n","mean       -0.643667      -0.730933     -2.588867     -3.731333     -2.661867   \n","std         9.102907       9.071642     23.554150     23.050615     23.536427   \n","min       -99.000000     -99.000000    -99.000000    -99.000000    -99.000000   \n","25%         0.000000       0.000000      1.000000      1.000000      1.000000   \n","50%         0.000000       0.000000      2.000000      1.000000      2.000000   \n","75%         0.000000       0.000000      3.000000      2.000000      3.000000   \n","max        23.000000      12.000000    165.000000     73.000000    165.000000   \n","\n","       WeblogInfo_7  WeblogInfo_8        target  \n","count  30000.000000  30000.000000  30000.000000  \n","mean      10.632800      0.657633      0.073267  \n","std       16.097588      2.622133      0.260578  \n","min        0.000000      0.000000      0.000000  \n","25%        2.000000      0.000000      0.000000  \n","50%        6.000000      0.000000      0.000000  \n","75%       13.000000      0.000000      0.000000  \n","max      722.000000     81.000000      1.000000  \n","\n","[8 rows x 171 columns]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":"final_data.describe()"},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":"#将运营商信息变为哑变量\ncat_feats = ['UserInfo_8']\nfinal_data = pd.get_dummies(final_data,columns=cat_feats,drop_first=True)\n\n#其余的文本型变量意义不明，暂时先删掉，建议大家可以尝试把他们做成哑变量加入模型训练\nfinal_data = final_data.select_dtypes(exclude=['object'])"},{"cell_type":"code","execution_count":21,"metadata":{"scrolled":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Education_Info1</th>\n","      <th>Idx</th>\n","      <th>SocialNetwork_10</th>\n","      <th>SocialNetwork_11</th>\n","      <th>SocialNetwork_12</th>\n","      <th>SocialNetwork_13</th>\n","      <th>SocialNetwork_14</th>\n","      <th>SocialNetwork_17</th>\n","      <th>SocialNetwork_3</th>\n","      <th>SocialNetwork_4</th>\n","      <th>...</th>\n","      <th>WeblogInfo_4</th>\n","      <th>WeblogInfo_5</th>\n","      <th>WeblogInfo_6</th>\n","      <th>WeblogInfo_7</th>\n","      <th>WeblogInfo_8</th>\n","      <th>target</th>\n","      <th>UserInfo_8_</th>\n","      <th>UserInfo_8_ChinaMobile</th>\n","      <th>UserInfo_8_ChinaTelecom</th>\n","      <th>UserInfo_8_ChinaUnicom</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>28564</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>17</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>89563</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>...</td>\n","      <td>6.0</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>50</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2748</td>\n","      <td>319</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>26</td>\n","      <td>55</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>64456</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>73732</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 175 columns</p>\n","</div>"],"text/plain":["   Education_Info1    Idx  SocialNetwork_10  SocialNetwork_11  \\\n","0                0  28564                -1                -1   \n","1                0  89563                -1                -1   \n","2                0   2748               319                -1   \n","3                0  64456                -1                -1   \n","4                1  73732                -1                -1   \n","\n","   SocialNetwork_12  SocialNetwork_13  SocialNetwork_14  SocialNetwork_17  \\\n","0                -1                 0                 0                 0   \n","1                -1                 0                 1                 0   \n","2                 0                 1                 0                 1   \n","3                -1                 0                 0                 0   \n","4                -1                 1                 0                 0   \n","\n","   SocialNetwork_3  SocialNetwork_4  ...  WeblogInfo_4  WeblogInfo_5  \\\n","0               -1               -1  ...           1.0           1.0   \n","1               -1               -1  ...           6.0           5.0   \n","2               26               55  ...           2.0           2.0   \n","3               -1               -1  ...           1.0           1.0   \n","4               -1               -1  ...           2.0           2.0   \n","\n","   WeblogInfo_6  WeblogInfo_7  WeblogInfo_8  target  UserInfo_8_   \\\n","0           1.0            17             2       0             0   \n","1           6.0            50             0       0             0   \n","2           2.0             0             0       0             0   \n","3           1.0             5             2       0             0   \n","4           2.0            13             0       0             0   \n","\n","   UserInfo_8_ChinaMobile  UserInfo_8_ChinaTelecom  UserInfo_8_ChinaUnicom  \n","0                       0                        0                       1  \n","1                       0                        1                       0  \n","2                       1                        0                       0  \n","3                       0                        0                       0  \n","4                       1                        0                       0  \n","\n","[5 rows x 175 columns]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":"final_data.head()"},{"cell_type":"code","execution_count":22,"metadata":{"scrolled":true},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":"# 为确保绘制的饼图为圆形，需执行如下代码\nplt.axes(aspect = 'equal')\n# 统计交易是否为欺诈的频数\ncounts = final_data.target.value_counts()\n\n# 绘制饼图\nplt.pie(x = counts, # 绘图数据\n        labels=pd.Series(counts.index).map({1:'default',0:'pay on time'}), # 添加文字标签\n        autopct='%.2f%%' # 设置百分比的格式，这里保留一位小数\n       )\n# 显示图形\nplt.show()"},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":"from sklearn.model_selection import train_test_split\nX = final_data.drop('target',axis=1)\ny = final_data['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)"},{"cell_type":"markdown","metadata":{},"source":["这里首先使用SMOTE方法，到这一步为止所有步骤都与第一章分类中的操作相同，大家可以根据自己之前的想法来调整特征工程"]},{"cell_type":"code","execution_count":24,"metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0    0.927238\n","1    0.072762\n","Name: target, dtype: float64\n","1    0.5\n","0    0.5\n","dtype: float64\n"]}],"source":"over_samples = SMOTE(random_state=666) \nover_samples_X,over_samples_y = over_samples.fit_sample(X_train, y_train)\n# 重抽样前的类别比例\nprint(y_train.value_counts()/len(y_train))\n# 重抽样后的类别比例\nprint(pd.Series(over_samples_y).value_counts()/len(over_samples_y))"},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":"def baseline_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(32, input_dim=174, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # Compile model\n    model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n    return model"},{"cell_type":"code","execution_count":47,"metadata":{"scrolled":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1\n","21000/21000 [==============================] - 0s 21us/step - loss: 243.0779 - accuracy: 0.8515\n"]},{"data":{"text/plain":["<keras.callbacks.callbacks.History at 0x21144070c50>"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":"estimator1 = KerasClassifier(build_fn=baseline_model, nb_epoch=40, batch_size=256)\nestimator1.fit(X_train, y_train)"},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1\n","8100/8100 [==============================] - 0s 21us/step - loss: 273.1389 - accuracy: 0.8570\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 20us/step - loss: 315.7835 - accuracy: 0.8564\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 20us/step - loss: 292.3687 - accuracy: 0.8480\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 23us/step - loss: 241.6186 - accuracy: 0.8642\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 19us/step - loss: 688.7099 - accuracy: 0.7921\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 19us/step - loss: 230.3607 - accuracy: 0.8504\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 20us/step - loss: 276.6615 - accuracy: 0.8149\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 21us/step - loss: 1064.4222 - accuracy: 0.7343\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 19us/step - loss: 730.7248 - accuracy: 0.7854\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 19us/step - loss: 314.9980 - accuracy: 0.8596\n"]}],"source":"from sklearn.model_selection import cross_val_predict\ny_pred = cross_val_predict(estimator1, X_test, y_test, cv=10)\nconf_mat = confusion_matrix(y_test, y_pred)"},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"data":{"text/plain":["array([[7844,  486],\n","       [ 611,   59]], dtype=int64)"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":"conf_mat"},{"cell_type":"code","execution_count":51,"metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1\n","38944/38944 [==============================] - 0s 10us/step - loss: 414.0160 - accuracy: 0.5525\n"]},{"data":{"text/plain":["<keras.callbacks.callbacks.History at 0x21157e42fd0>"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":"estimator2 = KerasClassifier(build_fn=baseline_model, nb_epoch=40, batch_size=256)\nestimator2.fit(over_samples_X,over_samples_y)"},{"cell_type":"code","execution_count":32,"metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.93      0.78      0.85      8330\n","           1       0.10      0.30      0.15       670\n","\n","    accuracy                           0.74      9000\n","   macro avg       0.52      0.54      0.50      9000\n","weighted avg       0.87      0.74      0.80      9000\n","\n","[[6501 1829]\n"," [ 469  201]]\n"]}],"source":"predictions2 = estimator2.predict(X_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predictions2))\nconfusion_matrix2 = confusion_matrix(y_test,predictions2)\nprint(confusion_matrix2)"},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1\n","8100/8100 [==============================] - 0s 19us/step - loss: 310.7305 - accuracy: 0.8310\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 21us/step - loss: 331.0739 - accuracy: 0.8165\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 20us/step - loss: 340.2750 - accuracy: 0.8589\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 20us/step - loss: 241.1645 - accuracy: 0.8359\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 20us/step - loss: 549.5516 - accuracy: 0.7701\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 21us/step - loss: 211.0534 - accuracy: 0.8504\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 23us/step - loss: 260.4284 - accuracy: 0.8651\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 21us/step - loss: 353.9604 - accuracy: 0.8231\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 25us/step - loss: 730.0018 - accuracy: 0.7649\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 26us/step - loss: 196.0868 - accuracy: 0.8633\n"]}],"source":"y_pred = cross_val_predict(estimator2, X_test, y_test, cv=10)\nconf_mat = confusion_matrix(y_test, y_pred)"},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"data":{"text/plain":["array([[7178, 1152],\n","       [ 560,  110]], dtype=int64)"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":"conf_mat"},{"cell_type":"markdown","metadata":{},"source":["这里发现有大量的不违约样本被预测成了违约，猜想可能是SMOTE方法生成的一部分正负样本在空间中过于接近，造成了过拟合，故采取SMOTE+Tomek link方法，来改善这一情况。\n","**SMOTE抽样 + Tomek Link**    \n","SMOTE的主要思想是利用特征空间中现存少数类样本之间的相似性来建立人工数据，特别是，对于子集Smin ⊂ S，对于每一个样本xi⊂Smin使用K-近邻法，其中K-近邻被定义为考虑Smin中的K个元素本身与xi的欧氏距离在n维特征空间X中表现为最小幅度值的样本。由于不是简单地复制少数类样本，因此可以在一定程度上避免分类器的过度拟合，实践证明此方法可以提高分类器的性能。     \n","但是由于对每个少数类样本都生成新样本，因此容易发生生成样本重叠（overlapping）的问题。因此在利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T后，引入ENN(Edited Nearest Neighbors)，对T中的每一个样本使用KNN（一般K取3）方法预测，若预测结果与实际类别标签不符，则剔除该样本。\n","https://imbalanced-learn.readthedocs.io/en/stable/combine.html#bbm2003"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original dataset shape Counter({0: 19472, 1: 1528})\n","Resampled dataset shape Counter({0: 19443, 1: 19443})\n"]}],"source":"print('Original dataset shape %s' % Counter(y_train))  #重抽样前的y值数量\nsmt = SMOTETomek(random_state=42)\nX_res, y_res = smt.fit_resample(X_train, y_train)\nprint('Resampled dataset shape %s' % Counter(y_res)) #重抽样后的y值数量"},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1\n","38886/38886 [==============================] - 0s 11us/step - loss: 379.2231 - accuracy: 0.5222\n"]},{"data":{"text/plain":["<keras.callbacks.callbacks.History at 0x2116d6b9fd0>"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":"estimator3 = KerasClassifier(build_fn=baseline_model, nb_epoch=40, batch_size=256)\nestimator3.fit(X_res, y_res)"},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1\n","8100/8100 [==============================] - 0s 19us/step - loss: 196.4262 - accuracy: 0.8535\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 19us/step - loss: 465.9692 - accuracy: 0.8456\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 20us/step - loss: 306.5214 - accuracy: 0.8341\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 20us/step - loss: 792.1055 - accuracy: 0.7752\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 21us/step - loss: 270.6456 - accuracy: 0.8707\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 21us/step - loss: 187.0553 - accuracy: 0.8641\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 20us/step - loss: 884.9254 - accuracy: 0.7735\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 20us/step - loss: 548.3796 - accuracy: 0.8206\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 23us/step - loss: 258.2597 - accuracy: 0.8660\n","Epoch 1/1\n","8100/8100 [==============================] - 0s 22us/step - loss: 209.2129 - accuracy: 0.8678\n"]}],"source":"y_pred = cross_val_predict(estimator3, X_test, y_test, cv=10)\nconf_mat = confusion_matrix(y_test, y_pred)"},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"data":{"text/plain":["array([[7795,  535],\n","       [ 604,   66]], dtype=int64)"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":"conf_mat"},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1\n","38886/38886 [==============================] - 0s 10us/step - loss: 574.2219 - accuracy: 0.5416\n"]},{"name":"stderr","output_type":"stream","text":["C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"]}],"source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nensemble=VotingClassifier(estimators=[('NN',KerasClassifier(build_fn=baseline_model, nb_epoch=40, batch_size=256)),\n                                              ('LR',LogisticRegression(C=0.05)),\n                                              ('DT',DecisionTreeClassifier(criterion = 'entropy')),\n                                              ('NB',GaussianNB()),\n                                             ], \n                       voting='soft').fit(X_res, y_res)"},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.94      0.79      0.86      8330\n","           1       0.11      0.32      0.16       670\n","\n","    accuracy                           0.76      9000\n","   macro avg       0.52      0.56      0.51      9000\n","weighted avg       0.87      0.76      0.80      9000\n","\n","[[6580 1750]\n"," [ 455  215]]\n"]}],"source":"predictions5 = ensemble.predict(X_test)\nprint(classification_report(y_test,predictions5))\nfrom sklearn.metrics import classification_report,confusion_matrix\nconfusion_matrix5 = confusion_matrix(y_test,predictions5)\nprint(confusion_matrix5)"},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TP, FN, FP, TN的值依次是: 6580 1750 455 215\n","优化前的收入为: 163000.0 优化后的收入为: 203000.0 ;相比之前，营收变化了 40000.0\n"]}],"source":"TP = confusion_matrix5[0,0]\nFN = confusion_matrix5[0,1] \nFP = confusion_matrix5[1,0]; TN = confusion_matrix5[1,1]\nprint (u\"TP, FN, FP, TN的值依次是:\", TP, FN, FP, TN)\nprint (u\"优化前的收入为:\",1000*(0.1*TP+0.1*FN-FP-TN),\"优化后的收入为:\",1000*(0.1*TP-FP),\";相比之前，营收变化了\",-1000*(0.1*TP+0.1*FN-FP-TN)+1000*(0.1*TP-FP))"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}